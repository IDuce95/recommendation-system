# Dockerfile for PySpark Processing
# 
# Ten Dockerfile tworzy Å›rodowisko z PySpark do distributed computing.
# Bazuje na oficjalnym obrazie Spark z Python support.

FROM apache/spark-py:v3.5.0

# Switch to root for installations
USER root

# Install system dependencies
RUN apt-get update && apt-get install -y \
    wget \
    curl \
    unzip \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements first (for better Docker caching)
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy our application code
COPY app/ app/
COPY config/ config/
COPY ai/ ai/

# Set Python path
ENV PYTHONPATH=/app:$PYTHONPATH

# Create directories for Spark
RUN mkdir -p /tmp/spark-events
RUN mkdir -p /app/spark_checkpoints
RUN mkdir -p /app/temp/spark

# Set Spark configuration
ENV SPARK_HOME=/opt/spark
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=python3

# Configure Spark defaults
COPY spark-defaults.conf $SPARK_HOME/conf/

# Expose Spark UI port
EXPOSE 4040

# Default command
CMD ["/opt/spark/bin/spark-submit", "--help"]
